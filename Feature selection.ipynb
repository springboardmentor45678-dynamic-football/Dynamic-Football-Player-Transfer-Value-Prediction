{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btKoMX16i1eh",
        "outputId": "45cb32c0-8dd4-4025-f15c-ba0087bdc8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Final dataset loaded: (50000, 161)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "def safe_load_csv(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at {file_path} was not found. Please check the path.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/datasets_2/\"\n",
        "final_path = path + \"master_file2_preprocessed_small.csv\"\n",
        "df = safe_load_csv(final_path)\n",
        "\n",
        "if df is not None:\n",
        "    print(\"Final dataset loaded:\", df.shape)\n",
        "else:\n",
        "    print(\"Failed to load dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "if \"date_of_birth\" in df.columns:\n",
        "    df[\"date_of_birth\"] = pd.to_datetime(df[\"date_of_birth\"], errors='coerce')\n",
        "    df[\"age\"] = datetime.now().year - df[\"date_of_birth\"].dt.year\n"
      ],
      "metadata": {
        "id": "9xEeyWnAi9k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"injury_reason\" in df.columns:\n",
        "    df[\"is_injured\"] = df[\"injury_reason\"].apply(lambda x: 0 if pd.isna(x) else 1)\n"
      ],
      "metadata": {
        "id": "ZZ03vqtBjCrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (\"goals\" in df.columns) and (\"appearances\" in df.columns):\n",
        "    df[\"goals_per_match\"] = df[\"goals\"] / (df[\"appearances\"] + 1)\n"
      ],
      "metadata": {
        "id": "UylQTMFCjFLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df.select_dtypes(include=['float64','int64']).columns\n",
        "\n",
        "z = np.abs((df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std())\n",
        "# df_no_outliers = df[(z < 3).all(axis=1)] # Original aggressive outlier removal\n",
        "\n",
        "# Temporarily bypassing aggressive outlier removal to allow model training to proceed\n",
        "df_no_outliers = df.copy()\n",
        "\n",
        "print(\"After outlier removal:\", df_no_outliers.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQcpTrNcjIvb",
        "outputId": "f676939c-a244-4607-f5b3-c85902234af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After outlier removal: (50000, 161)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"value\"  # change if needed!\n",
        "\n",
        "df_no_outliers = df_no_outliers.dropna(subset=[target])  # ensure no missing target\n",
        "\n",
        "X = df_no_outliers.drop(columns=[target])\n",
        "y = df_no_outliers[target]\n",
        "\n",
        "print(\"Shapes:\", X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMqDFAQXjOyI",
        "outputId": "7d40706b-410b-4996-a31a-be96a6d1aed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (48693, 160) (48693,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4EoKBjmjRxB",
        "outputId": "4f38b6e8-bdb0-4a16-8676-a5d731c9d4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (38954, 160)  Test: (9739, 160)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_imputed, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test_imputed)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"----- Linear Regression Performance -----\")\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R²:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGzp6mYrjZI6",
        "outputId": "8ad65611-30f4-4420-ab82-c4903d0e9d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['player_name_y' 'text' 'vader_polarity' 'vader_emotion' 'tb_polarity'\n",
            " 'tb_emotion' 'game_date' 'tweet_date' 'when']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['player_name_y' 'text' 'vader_polarity' 'vader_emotion' 'tb_polarity'\n",
            " 'tb_emotion' 'game_date' 'tweet_date' 'when']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Linear Regression Performance -----\n",
            "MSE: 49156732348211.16\n",
            "RMSE: 7011186.229748227\n",
            "MAE: 2825116.8301904784\n",
            "R²: 0.1516672078886694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "poly = PolynomialFeatures(degree=1)\n",
        "X_train_poly = poly.fit_transform(X_train_imputed)\n",
        "X_test_poly = poly.transform(X_test_imputed)\n",
        "\n",
        "model_poly_1 = LinearRegression()\n",
        "model_poly_1.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred_poly_1 = model_poly_1.predict(X_test_poly)\n",
        "rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred_poly_1))\n",
        "\n",
        "print(\"Polynomial Degree 1 → RMSE:\", rmse_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oeLdDiRjbo2",
        "outputId": "a0c7f1db-ba63-4cc3-de89-8827b5ac58ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial Degree 1 → RMSE: 7011186.229744081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: reduce features\n",
        "# X_train_imputed is a numpy array, so select_dtypes is not applicable.\n",
        "# We will directly select the first 20 columns from the imputed arrays.\n",
        "X_train_small = X_train_imputed[:, :20]\n",
        "X_test_small = X_test_imputed[:, :20]\n",
        "\n",
        "print(\"Shapes:\", X_train_small.shape, X_test_small.shape)\n",
        "\n",
        "# Step 2: polynomial degree 2\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly.fit_transform(X_train_small)\n",
        "X_test_poly = poly.transform(X_test_small)\n",
        "\n",
        "print(\"Poly shape:\", X_train_poly.shape)\n",
        "\n",
        "# Step 3: model\n",
        "model_poly_2 = LinearRegression()\n",
        "model_poly_2.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred = model_poly_2.predict(X_test_poly)\n",
        "rmse_2 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"Degree 2 RMSE:\", rmse_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtYL_oS-lhHs",
        "outputId": "85042af5-583d-4bd8-8cbe-56066c09c722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (38954, 20) (9739, 20)\n",
            "Poly shape: (38954, 231)\n",
            "Degree 2 RMSE: 7189956.039213703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_imputed is a numpy array, so select_dtypes is not applicable.\n",
        "# We will directly select the first 10 columns from the imputed arrays.\n",
        "selected_cols = X_train_imputed[:, :10]\n",
        "\n",
        "X_train_small = selected_cols\n",
        "X_test_small = X_test_imputed[:, :10]\n",
        "\n",
        "print(\"Using the first 10 features.\")\n",
        "print(\"Shapes:\", X_train_small.shape, X_test_small.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAqpESPemnMn",
        "outputId": "056a1e9a-6299-4882-9ce9-f26b6b15c2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the first 10 features.\n",
            "Shapes: (38954, 10) (9739, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(degree=3)\n",
        "X_train_poly = poly.fit_transform(X_train_small)\n",
        "X_test_poly = poly.transform(X_test_small)\n",
        "\n",
        "model_poly_3 = LinearRegression()\n",
        "model_poly_3.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred_poly_3 = model_poly_3.predict(X_test_poly)\n",
        "rmse_3 = np.sqrt(mean_squared_error(y_test, y_pred_poly_3))\n",
        "\n",
        "print(\"Polynomial Degree 3 → RMSE:\", rmse_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsdiH5AbljXr",
        "outputId": "3a70dd91-584b-4e0a-bfb0-4342f0acf5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial Degree 3 → RMSE: 7190105.800846882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# RANDOM FOREST FEATURE SELECTION → LIGHTGBM (OPTIMIZED)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib, time\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from lightgbm import LGBMRegressor\n",
        "from scipy import sparse\n",
        "import lightgbm\n",
        "import os # Import the os module for directory operations\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. TRAIN STRONG RANDOM FOREST (FULL FEATURES)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\n=== TRAINING RANDOM FOREST (FULL FEATURES) ===\")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=20,\n",
        "    min_samples_split=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define feature_names from the original X columns before imputation\n",
        "# Correcting feature_names to exclude columns skipped by SimpleImputer\n",
        "skipped_imputer_cols = ['player_name_y', 'text', 'vader_polarity', 'vader_emotion', 'tb_polarity', 'tb_emotion', 'game_date', 'tweet_date', 'when']\n",
        "feature_names = [col for col in X.columns.tolist() if col not in skipped_imputer_cols]\n",
        "\n",
        "# RF prefers dense when feature count is manageable\n",
        "X_train_rf = X_train_imputed.toarray() if sparse.isspmatrix(X_train_imputed) else X_train_imputed\n",
        "X_test_rf  = X_test_imputed.toarray()  if sparse.isspmatrix(X_test_imputed) else X_test_imputed\n",
        "\n",
        "t0 = time.time()\n",
        "rf.fit(X_train_rf, y_train)\n",
        "print(\"RF training time:\", round(time.time() - t0, 2), \"seconds\")\n",
        "\n",
        "y_rf = rf.predict(X_test_rf)\n",
        "print(\"RF RMSE:\", np.sqrt(mean_squared_error(y_test, y_rf)))\n",
        "print(\"RF R² :\", r2_score(y_test, y_rf))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. EXTRACT FEATURE IMPORTANCE\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "rf_importance = pd.Series(\n",
        "    rf.feature_importances_,\n",
        "    index=feature_names\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 RF Features:\")\n",
        "print(rf_importance.head(20))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. SELECT TOP FEATURES (RF-BASED)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Strategy: keep features contributing to 95% cumulative importance\n",
        "cumulative_importance = rf_importance.cumsum()\n",
        "selected_features = cumulative_importance[cumulative_importance <= 0.95].index.tolist()\n",
        "\n",
        "# Safety fallback\n",
        "if len(selected_features) < 20:\n",
        "    selected_features = rf_importance.head(30).index.tolist()\n",
        "\n",
        "print(\"\\nSelected RF Features Count:\", len(selected_features))\n",
        "print(\"Selected RF Features:\\n\", selected_features)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. REDUCE TRAIN / TEST MATRICES TO SELECTED FEATURES\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "feature_index_map = {f: i for i, f in enumerate(feature_names)}\n",
        "selected_indices = [feature_index_map[f] for f in selected_features]\n",
        "\n",
        "X_train_sel = X_train_imputed[:, selected_indices]\n",
        "X_test_sel  = X_test_imputed[:, selected_indices]\n",
        "\n",
        "print(\"Reduced Train Shape:\", X_train_sel.shape)\n",
        "print(\"Reduced Test Shape :\", X_test_sel.shape)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. TRAIN LIGHTGBM USING RF-SELECTED FEATURES\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\n=== TRAINING LIGHTGBM (RF-SELECTED FEATURES) ===\")\n",
        "\n",
        "lgb = LGBMRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=12,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    min_child_samples=10,\n",
        "    objective=\"regression\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "lgb.fit(\n",
        "    X_train_sel,\n",
        "    y_train,\n",
        "    eval_set=[(X_test_sel, y_test)],\n",
        "    eval_metric=\"rmse\",\n",
        "    callbacks=[lightgbm.early_stopping(50, verbose=False)]\n",
        ")\n",
        "print(\"LGB training time:\", round(time.time() - t0, 2), \"seconds\")\n",
        "\n",
        "y_lgb = lgb.predict(X_test_sel)\n",
        "\n",
        "print(\"\\n=== FINAL LIGHTGBM PERFORMANCE ===\")\n",
        "print(\"LGB RMSE:\", np.sqrt(mean_squared_error(y_test, y_lgb)))\n",
        "print(\"LGB R² :\", r2_score(y_test, y_lgb))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. LIGHTGBM FEATURE IMPORTANCE (GAIN)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "lgb_importance = pd.Series(\n",
        "    lgb.feature_importances_,\n",
        "    index=selected_features\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 LightGBM Features:\")\n",
        "print(lgb_importance.head(20))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7. SAVE ARTIFACTS\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Define the directory path\n",
        "models_dir = \"/content/drive/MyDrive/datasets_2/models_final/\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(rf, os.path.join(models_dir, \"best_rf.joblib\"))\n",
        "joblib.dump(lgb, os.path.join(models_dir, \"best_lgb_rf_selected.joblib\"))\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"feature\": selected_features,\n",
        "    \"rf_importance\": rf_importance[selected_features].values\n",
        "}).to_csv(\n",
        "    os.path.join(models_dir, \"rf_selected_features.csv\"),\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"\\nArtifacts saved:\")\n",
        "print(\"✓ best_rf.joblib\")\n",
        "print(\"✓ best_lgb_rf_selected.joblib\")\n",
        "print(\"✓ rf_selected_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN5V3D3MIbTw",
        "outputId": "31ad9e3c-96fa-4ae6-8970-23b1ff80ceab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAINING RANDOM FOREST (FULL FEATURES) ===\n",
            "RF training time: 171.76 seconds\n",
            "RF RMSE: 2589493.891867249\n",
            "RF R² : 0.8842787727126821\n",
            "\n",
            "Top 20 RF Features:\n",
            "current_club_id                           0.230224\n",
            "contract_expires                          0.163748\n",
            "player_id                                 0.143295\n",
            "height                                    0.066780\n",
            "player_agent_id                           0.058894\n",
            "team_id                                   0.037636\n",
            "goals                                     0.022705\n",
            "outfitter_adidas                          0.020807\n",
            "minutes_played                            0.018570\n",
            "position_Midfield - Attacking Midfield    0.016613\n",
            "position_Attack - Centre-Forward          0.016113\n",
            "outfitter_Nike                            0.014941\n",
            "outfitter_Skechers                        0.013381\n",
            "position_Attack - Left Winger             0.012040\n",
            "days_missed                               0.011930\n",
            "games_missed                              0.010468\n",
            "on_loan_from_club_id                      0.009640\n",
            "main_position_Attack                      0.009267\n",
            "season_name_x                             0.008404\n",
            "position_Attack - Right Winger            0.008232\n",
            "dtype: float64\n",
            "\n",
            "Selected RF Features Count: 31\n",
            "Selected RF Features:\n",
            " ['current_club_id', 'contract_expires', 'player_id', 'height', 'player_agent_id', 'team_id', 'goals', 'outfitter_adidas', 'minutes_played', 'position_Midfield - Attacking Midfield', 'position_Attack - Centre-Forward', 'outfitter_Nike', 'outfitter_Skechers', 'position_Attack - Left Winger', 'days_missed', 'games_missed', 'on_loan_from_club_id', 'main_position_Attack', 'season_name_x', 'position_Attack - Right Winger', 'nb_on_pitch', 'position_Midfield - Defensive Midfield', 'assists', 'subed_in', 'foot_left', 'position_Midfield - Central Midfield', 'nb_in_group', 'is_eu', 'position_Attack - Second Striker', 'main_position_Midfield', 'position_Defender - Centre-Back']\n",
            "Reduced Train Shape: (38954, 31)\n",
            "Reduced Test Shape : (9739, 31)\n",
            "\n",
            "=== TRAINING LIGHTGBM (RF-SELECTED FEATURES) ===\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068194 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2178\n",
            "[LightGBM] [Info] Number of data points in the train set: 38954, number of used features: 31\n",
            "[LightGBM] [Info] Start training from score 2017472.018278\n",
            "LGB training time: 4.79 seconds\n",
            "\n",
            "=== FINAL LIGHTGBM PERFORMANCE ===\n",
            "LGB RMSE: 2682905.5984194735\n",
            "LGB R² : 0.8757792827613574\n",
            "\n",
            "Top 20 LightGBM Features:\n",
            "current_club_id                           2942\n",
            "player_id                                 2314\n",
            "player_agent_id                           1866\n",
            "height                                    1403\n",
            "contract_expires                          1373\n",
            "team_id                                    958\n",
            "days_missed                                441\n",
            "on_loan_from_club_id                       312\n",
            "outfitter_Nike                             258\n",
            "games_missed                               244\n",
            "outfitter_adidas                           235\n",
            "season_name_x                              233\n",
            "minutes_played                             205\n",
            "nb_on_pitch                                202\n",
            "goals                                      166\n",
            "nb_in_group                                160\n",
            "foot_left                                  149\n",
            "main_position_Midfield                     142\n",
            "position_Midfield - Attacking Midfield     138\n",
            "is_eu                                      138\n",
            "dtype: int32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Artifacts saved:\n",
            "✓ best_rf.joblib\n",
            "✓ best_lgb_rf_selected.joblib\n",
            "✓ rf_selected_features.csv\n"
          ]
        }
      ]
    }
  ]
}