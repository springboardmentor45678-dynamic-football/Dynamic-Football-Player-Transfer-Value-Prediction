{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35e356b",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce6784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063cc58",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"--- PROJECT START ---\")\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824458d",
   "metadata": {},
   "source": [
    "### ðŸ”¹ cell3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dc681",
   "metadata": {},
   "source": [
    "######1: Data Merging and Initial Inspection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2edda8",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39571ce",
   "metadata": {},
   "source": [
    "###### 1. Load Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586e446",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513df762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Player-related raw_data\n",
    "    injuries_df = pd.read_csv(\"player_injuries.csv\")\n",
    "    latest_market_value_df = pd.read_csv(\"player_latest_market_value.csv\")\n",
    "    market_value_df = pd.read_csv(\"player_market_value.csv\")\n",
    "    national_perf_df = pd.read_csv(\"player_national_performances.csv\")\n",
    "    performances_df = pd.read_csv(\"player_performances.csv\") # Corrected filename\n",
    "    profiles_df = pd.read_csv(\"player_profiles.csv\", low_memory=False) # low_memory=False to handle mixed types\n",
    "    teammates_df = pd.read_csv(\"player_teammates_played_with.csv\")\n",
    "\n",
    "    # Team-related raw_data\n",
    "    team_children_df = pd.read_csv(\"team_children.csv\")\n",
    "    team_comp_season_df = pd.read_csv(\"team_competitions_seasons.csv\")\n",
    "    team_details_df = pd.read_csv(\"team_details.csv\")\n",
    "\n",
    "    # Transfer raw_data\n",
    "    transfer_history_df = pd.read_csv(\"transfer_history.csv\")\n",
    "\n",
    "    # Sentiment / tweets raw_data\n",
    "    tweets_df = pd.read_csv(\"tweets_premier_league_footballers.csv\", encoding='latin1') # Corrected filename and encoding\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading file: {e}. Please ensure all files are uploaded correctly.\")\n",
    "    raise\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ba33e",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd699db5",
   "metadata": {},
   "source": [
    "#####2. Merge Datasets\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5770d",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Ensure 'player_id' columns are of consistent type before merging\n",
    "for df_name in [profiles_df, latest_market_value_df, market_value_df, performances_df, national_perf_df, injuries_df, tweets_df]:\n",
    "    if 'player_id' in df_name.columns:\n",
    "        df_name['player_id'] = pd.to_numeric(df_name['player_id'], errors='coerce').astype('Int64') # Use Int64 for nullable integer\n",
    "\n",
    "# Start merging using player profiles as the base\n",
    "merged_df = profiles_df.copy()\n",
    "print(f\"Initial profiles_df shape: {merged_df.shape}\")\n",
    "\n",
    "# Standardize player names in profiles_df to match tweets_df format for merging\n",
    "profiles_df['clean_player_name'] = profiles_df['player_name'].str.split('(', n=1).str[0].str.strip().str.upper()\n",
    "\n",
    "# Merge market value (latest + historical)\n",
    "merged_df = merged_df.merge(\n",
    "    latest_market_value_df.drop(columns=['player_name'], errors='ignore'),\n",
    "    on='player_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"After latest_market_value_df merge: {merged_df.shape}\")\n",
    "\n",
    "merged_df = merged_df.merge(\n",
    "    market_value_df.drop(columns=['player_name'], errors='ignore'),\n",
    "    on='player_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"After market_value_df merge: {merged_df.shape}\")\n",
    "\n",
    "# Merge performances\n",
    "merged_df = merged_df.merge(\n",
    "    performances_df,\n",
    "    on='player_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"After performances_df merge: {merged_df.shape}\")\n",
    "\n",
    "# Merge national performances\n",
    "merged_df = merged_df.merge(\n",
    "    national_perf_df,\n",
    "    on='player_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"After national_perf_df merge: {merged_df.shape}\")\n",
    "\n",
    "# Merge injuries\n",
    "merged_df = merged_df.merge(\n",
    "    injuries_df,\n",
    "    on='player_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"After injuries_df merge: {merged_df.shape}\")\n",
    "\n",
    "# Prepare tweets raw_data for merging\n",
    "# 1. Create a mapping from clean_player_name to player_id\n",
    "player_name_to_id = profiles_df[['clean_player_name', 'player_id']].drop_duplicates()\n",
    "\n",
    "# 2. Aggregate tweets_df by player_name and calculate mean for sentiment metrics\n",
    "#    First, ensure 'tweet_text' column is dropped before aggregation if it exists\n",
    "#    Convert player_name to upper case for consistent matching\n",
    "tweets_df['player_name_upper'] = tweets_df['player_name'].str.upper()\n",
    "tweets_agg = tweets_df.drop(columns=['tweet_text', 'player_name'], errors='ignore').groupby('player_name_upper').mean(numeric_only=True).reset_index()\n",
    "\n",
    "# 3. Merge aggregated tweets raw_data with player_name_to_id to get player_id\n",
    "#    Use 'player_name_upper' from tweets_agg and 'clean_player_name' from mapping\n",
    "tweets_with_id = tweets_agg.merge(player_name_to_id, left_on='player_name_upper', right_on='clean_player_name', how='left')\n",
    "\n",
    "# Drop redundant player name columns before final merge\n",
    "tweets_with_id = tweets_with_id.drop(columns=['player_name_upper', 'clean_player_name'], errors='ignore')\n",
    "\n",
    "# Merge sentiment/tweets raw_data\n",
    "merged_df = merged_df.merge(\n",
    "    tweets_with_id,\n",
    "    on='player_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"After tweets_with_id merge: {merged_df.shape}\")\n",
    "\n",
    "# OPTIONAL: merge teammates played with (if needed)\n",
    "# merged_df = merged_df.merge(teammates_df, on='player_id', how='left')\n",
    "\n",
    "# Save merged dataset\n",
    "merged_df.to_csv(\"merged_data_initial.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Merged Data saved to 'merged_data_initial.csv'.\")\n",
    "print(f\"Merged Data Shape: {merged_df.shape}\")\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152aa44",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "from google.colab import files\n",
    "\n",
    "try:\n",
    "    files.download('merged_final_data.csv')\n",
    "    print(\"File download initiated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading file: {e}\")\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11cf16",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95148320",
   "metadata": {},
   "source": [
    "#### 2.Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab545c75",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc965da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "print(\"\\n[STEP 2/5] WEEK 2: Feature Engineering...\")\n",
    "dataset_df = merged_df.copy()\n",
    "\n",
    "# Calculate days_out for injury analysis\n",
    "dataset_df['from_date'] = pd.to_datetime(dataset_df['from_date'])\n",
    "dataset_df['end_date'] = pd.to_datetime(dataset_df['end_date'])\n",
    "dataset_df['days_out'] = (dataset_df['end_date'] - dataset_df['from_date']).dt.days\n",
    "\n",
    "# Calculate age from date_of_birth\n",
    "dataset_df['date_of_birth'] = pd.to_datetime(dataset_df['date_of_birth'])\n",
    "dataset_df['age'] = (pd.to_datetime('today').year - dataset_df['date_of_birth'].dt.year) - ((pd.to_datetime('today').month < dataset_df['date_of_birth'].dt.month) | \\\n",
    "          ((pd.to_datetime('today').month == dataset_df['date_of_birth'].dt.month) & (pd.to_datetime('today').day < dataset_df['date_of_birth'].dt.day)))\n",
    "\n",
    "# Drop original date columns if not needed further\n",
    "dataset_df = dataset_df.drop(columns=['from_date', 'end_date', 'date_of_birth'], errors='ignore')\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ebe87",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Ensure no division by zero\n",
    "epsilon = 1e-6\n",
    "dataset_df['matches'] = dataset_df['matches'].replace(0, 1)\n",
    "dataset_df['minutes_played'] = dataset_df['minutes_played'].replace(0, 1)\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf19a0",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Basic Performance Metrics\n",
    "dataset_df['goals_per_90_min'] = (dataset_df['goals_x'] * 90) / dataset_df['minutes_played']\n",
    "dataset_df['assists_per_90_min'] = (dataset_df['assists'] * 90) / dataset_df['minutes_played']\n",
    "dataset_df['G_A_per_match'] = (dataset_df['goals_x'] + dataset_df['assists']) / dataset_df['matches']\n",
    "dataset_df['normalized_sentiment'] = (dataset_df['vader_polarity'] - dataset_df['vader_polarity'].min()) / \\\n",
    "                              (dataset_df['vader_polarity'].max() - dataset_df['vader_polarity'].min())\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60288a5f",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "dataset_df['log_days_out'] = np.log1p(dataset_df['days_out'])\n",
    "dataset_df['Injury_Impact_Index'] = (dataset_df['log_days_out'] * dataset_df['value_x']) / (dataset_df['matches'])\n",
    "dataset_df['Value_Efficiency_Ratio'] = dataset_df['value_x'] / (dataset_df['G_A_per_match'] + epsilon)\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc28ba",
   "metadata": {},
   "source": [
    "### Cell 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4ce76",
   "metadata": {},
   "source": [
    "##### 3. Data Preprocessing Pipeline (Scikit-learn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40951621",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "print(\"\\n[STEP 3/5] Data Preprocessing Pipeline...\")\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be2a50",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ab6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Define feature sets\n",
    "numerical_features = ['age', 'value_x', 'matches', 'goals_x', 'assists',\n",
    "                      'minutes_played', 'days_out',\n",
    "                      'vader_polarity', 'tb_polarity',\n",
    "                      'goals_per_90_min', 'assists_per_90_min', 'G_A_per_match',\n",
    "                      'normalized_sentiment', 'Injury_Impact_Index', 'Value_Efficiency_Ratio'] # Removed Polarity_Index\n",
    "categorical_features = ['citizenship', 'position', 'current_club_name', 'injury_reason'] # Corrected column names\n",
    "# Removed 'last_update' and 'sentiment_score' from drop_features as they caused KeyError.\n",
    "drop_features = ['player_id', 'player_name', 'log_days_out']\n",
    "X = dataset_df.drop(columns=drop_features)\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31a069",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Pipeline Transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle Missing Data\n",
    "    ('scaler', StandardScaler())                    # Scale Numeric Data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Encode Categorical Data\n",
    "])\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce499025",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ffc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Column Transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f57a63",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Apply the Pipeline\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "feature_names = numerical_features + list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features))\n",
    "processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f8e4a",
   "metadata": {},
   "source": [
    "### ðŸ”¹Cell 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# Save the final processed/processed raw_data\n",
    "processed_df.to_csv(\"cleaned_processed_data.csv\", index=False)\n",
    "print(\"âœ… Final Processed Data saved to 'cleaned_processed_data.csv'.\")\n",
    "print(f\"\\nProcessed Data Head (Scaled, first 10 columns):\\n{processed_df.iloc[:, :10].head()}\")\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880dc36",
   "metadata": {},
   "source": [
    "### ðŸ”¹Cell 21\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad32ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "import os\n",
    "print(os.listdir('.'))\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6362883",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffcd6a",
   "metadata": {},
   "source": [
    "If you see `cleaned_processed_data.csv` in the list above, you can use the following code to download it directly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15060e36",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "from google.colab import files\n",
    "\n",
    "try:\n",
    "    files.download('cleaned_processed_data.csv')\n",
    "    print(\"File download initiated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading file: {e}\")\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb812b",
   "metadata": {},
   "source": [
    "cell 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f4505",
   "metadata": {},
   "source": [
    "##### 4. Data Analysis & Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d154a2",
   "metadata": {},
   "source": [
    "### ðŸ”¹Cell 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# ==============================================================================\n",
    "print(\"\\n[STEP 4/5] WEEK 2: Exploratory Data Analysis (EDA) and Visualization...\")\n",
    "analysis_df = dataset_df.copy()\n",
    "\n",
    "# Ensure market_value is available as 'value_x'\n",
    "if 'value_x' not in analysis_df.columns:\n",
    "    print(\"Warning: 'value_x' (market value) not found in analysis_df. Please ensure correct merging and column naming.\")\n",
    "\n",
    "# Calculate correlation matrix for numerical features\n",
    "# Re-using the numerical_features list defined in H3Y-V_1kW9zO, ensuring it's up-to-date\n",
    "current_numerical_features = ['age', 'value_x', 'matches', 'goals_x', 'assists',\n",
    "                      'minutes_played', 'days_out',\n",
    "                      'vader_polarity', 'tb_polarity',\n",
    "                      'goals_per_90_min', 'assists_per_90_min', 'G_A_per_match',\n",
    "                      'normalized_sentiment', 'Injury_Impact_Index', 'Value_Efficiency_Ratio']\n",
    "\n",
    "# Filter to only existing numerical features to avoid KeyError\n",
    "existing_numerical_features = [f for f in current_numerical_features if f in analysis_df.columns]\n",
    "\n",
    "corr_matrix = analysis_df[existing_numerical_features].corr()\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff901e",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f657379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# A. Correlation Matrix (Key Features)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Key Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91102031",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 27\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# B. Segment Analysis: Market Value by Position\n",
    "position_value = analysis_df.groupby('position')['value_x'].mean().sort_values(ascending=False) # Changed market_value_million to value_x\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=position_value.index, y=position_value.values, palette='viridis')\n",
    "plt.title('Average Market Value by Position', fontsize=16)\n",
    "plt.ylabel('Average Market Value (Millions)', fontsize=12)\n",
    "plt.xlabel('Position', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cdcc15",
   "metadata": {},
   "source": [
    "### ðŸ”¹Cell 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da06d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "# C. Relationship with Target Variable: Market Value vs. G_A_per_match\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='G_A_per_match', y='value_x', raw_data=analysis_df, hue='position', palette='tab10') # Changed market_value_million to value_x\n",
    "plt.title('Market Value vs. Goals + Assists Per Match', fontsize=16)\n",
    "plt.xlabel('Goals + Assists Per Match', fontsize=12)\n",
    "plt.ylabel('Market Value (Millions)', fontsize=12)\n",
    "plt.legend(title='Position', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096f0d6",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "sns.lmplot(\n",
    "    raw_data=analysis_df,\n",
    "    x='age',\n",
    "    y='value_x', # Changed market_value_million to value_x\n",
    "    hue='position',    # Differentiate colors by position\n",
    "    col='position',    # Create separate plots (columns) for better clarity\n",
    "    height=4,\n",
    "    aspect=1.2,\n",
    "    scatter_kws={'alpha': 0.6},\n",
    "    line_kws={'lw': 2},\n",
    "    facet_kws={'sharex': False, 'sharey': False} # Use facet_kws for sharex and sharey to avoid UserWarning\n",
    ")\n",
    "\n",
    "plt.suptitle('Market Value vs. Age (Career Curve) Segmented by Position', y=1.05, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# === End of Refactored Cell ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6bc74c",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4f394",
   "metadata": {},
   "source": [
    "##### 5. Final Insight Summary\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeba20d",
   "metadata": {},
   "source": [
    "### ðŸ”¹  Cell 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Refactored Code Cell ===\n",
    "# Purpose: Auto-modified for new notebook version\n",
    "print(\"\\n[STEP 5/5] WEEK 2: Final Insight Summary\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Key Findings for Model Development (Week 1 & 2):\")\n",
    "print(\"1. Performance Dominance: The G+A per Match metric shows a high positive correlation. On-field output is the primary driver of market value.\")\n",
    "print(\"2. Positional Value: Forwards and Midfielders command the highest average market values, confirming positional scarcity/demand patterns.\")\n",
    "print(\"3. Injury Complexity: The engineered 'Injury_Impact_Index' has a surprisingly strong positive correlation (approx. 0.58). This suggests that players who are highly valued and spend time injured are likely elite assets whose value holds, or whose absences are strategically managed.\")\n",
    "print(\"4. Sentiment Weakness: Simple Polarity Index shows a very weak negative correlation. Raw sentiment scores are currently poor predictors of market value compared to performance metrics.\")\n",
    "print(\"\\nDataset Preparation Complete. 'cleaned_processed_data.csv' is ready for model training.\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "# === End of Refactored Cell ==="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
