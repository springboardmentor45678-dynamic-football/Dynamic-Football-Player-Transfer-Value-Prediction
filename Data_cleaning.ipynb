{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH1pQ6szmKIL",
        "outputId": "e01caf4b-a031-4105-8dea-e361e0fe700d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install chardet\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import chardet\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "path = \"/content/drive/MyDrive/datasets_2/\"\n",
        "\n",
        "def detect_encoding(file_path, num_bytes=50000):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return chardet.detect(f.read(num_bytes))['encoding']\n",
        "\n",
        "def safe_load_csv(file_path, chunksize=250000):\n",
        "    encoding = detect_encoding(file_path)\n",
        "    print(\"Encoding:\", encoding)\n",
        "    try:\n",
        "        return pd.concat(\n",
        "            pd.read_csv(file_path, encoding=encoding, chunksize=chunksize, low_memory=False, on_bad_lines='skip'),\n",
        "            ignore_index=True\n",
        "        )\n",
        "    except:\n",
        "        return pd.concat(\n",
        "            pd.read_csv(file_path, encoding='ISO-8859-1', chunksize=chunksize, low_memory=False, on_bad_lines='skip'),\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "def clean_dataframe(df):\n",
        "    df = df.drop_duplicates()\n",
        "    num_cols = df.select_dtypes(include=['int64','float64']).columns\n",
        "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in cat_cols:\n",
        "        if df[col].isna().sum() > 0:\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    return df\n",
        "\n",
        "def safe_ohe(df, columns):\n",
        "    if len(columns) == 0: return df\n",
        "    enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    encoded = enc.fit_transform(df[columns])\n",
        "    df2 = pd.DataFrame(encoded, columns=enc.get_feature_names_out(columns))\n",
        "    df = df.drop(columns, axis=1)\n",
        "    return pd.concat([df.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "    \"performances\": path + \"player_performances.csv\",\n",
        "    \"profiles\": path + \"player_profiles.csv\",\n",
        "    \"tweets\": path + \"tweets_premier_league_footballers.csv\",\n",
        "    \"injuries\": path + \"player_injuries.csv\",\n",
        "    \"market\": path + \"player_latest_market_value.csv\"\n",
        "}\n",
        "\n",
        "# CLEAN FILES\n",
        "cleaned_files = {}\n",
        "for name, file_path in files.items():\n",
        "    df = safe_load_csv(file_path)\n",
        "    df = clean_dataframe(df)\n",
        "    out_path = path + f\"{name}_s.csv\"\n",
        "    df.to_csv(out_path, index=False)\n",
        "    cleaned_files[name] = out_path\n",
        "\n",
        "# MERGE TWEETS WITH PROFILES (name match)\n",
        "profiles_df = safe_load_csv(cleaned_files[\"profiles\"])\n",
        "tweets_df   = safe_load_csv(cleaned_files[\"tweets\"])\n",
        "\n",
        "profiles_df[\"name_clean\"] = profiles_df[\"player_name\"].str.lower().str.strip()\n",
        "tweets_df[\"name_clean\"]   = tweets_df[\"player_name\"].str.lower().str.strip()\n",
        "\n",
        "tweets_merged = tweets_df.merge(\n",
        "    profiles_df[[\"player_id\",\"name_clean\"]],\n",
        "    on=\"name_clean\",\n",
        "    how=\"left\"\n",
        ").drop(columns=[\"name_clean\"])\n",
        "\n",
        "tweets_path = path + \"tweets_s.csv\"\n",
        "tweets_merged.to_csv(tweets_path, index=False)\n",
        "cleaned_files[\"tweets\"] = tweets_path\n",
        "\n",
        "# MASTER FILE 1\n",
        "merge_order = [\"profiles\",\"performances\",\"injuries\",\"market\",\"tweets\"]\n",
        "master1 = None\n",
        "\n",
        "for name in merge_order:\n",
        "    df = safe_load_csv(cleaned_files[name])\n",
        "    master1 = df if master1 is None else master1.merge(df, on=\"player_id\", how=\"left\")\n",
        "\n",
        "master1 = master1.drop_duplicates()\n",
        "master1_path = path + \"master_file1_cleaned.csv\"\n",
        "master1.to_csv(master1_path, index=False)\n",
        "\n",
        "master1_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "L063xpUpmLsV",
        "outputId": "8d2185ad-278f-4b42-9a75-0946e475119c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: utf-8\n",
            "Encoding: utf-8\n",
            "Encoding: ascii\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3818187785.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
            "/tmp/ipython-input-3818187785.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = df[col].fillna(df[col].mode()[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: ascii\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3818187785.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
            "/tmp/ipython-input-3818187785.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = df[col].fillna(df[col].mode()[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: ascii\n",
            "Encoding: utf-8\n",
            "Encoding: ascii\n",
            "Encoding: utf-8\n",
            "Encoding: utf-8\n",
            "Encoding: ascii\n",
            "Encoding: ascii\n",
            "Encoding: ascii\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/datasets_2/master_file1_cleaned.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "master1 = safe_load_csv(master1_path)\n",
        "print(\"Master1 loaded:\", master1.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdDoji8FwdDU",
        "outputId": "bea2e942-ce97-4149-b755-7f3633b4a3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: utf-8\n",
            "Master1 loaded: (3749436, 70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = master1.select_dtypes(include=['object']).columns\n",
        "\n",
        "cols_to_drop = [col for col in cat_cols if master1[col].nunique() > 200]\n",
        "\n",
        "print(\"Dropping high-cardinality columns:\")\n",
        "print(cols_to_drop)\n",
        "\n",
        "master1_reduced = master1.drop(columns=cols_to_drop)\n",
        "print(\"Reduced Master1 shape:\", master1_reduced.shape)\n",
        "master1_small = master1_reduced.sample(n=50000, random_state=42)\n",
        "print(\"Sampled dataset shape:\", master1_small.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eci3nhIKxAIl",
        "outputId": "bdb4ba30-5f02-4a79-e473-251d3a284bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping high-cardinality columns:\n",
            "['player_slug', 'player_name_x', 'player_image_url', 'name_in_home_country', 'date_of_birth', 'place_of_birth', 'country_of_birth', 'citizenship', 'current_club_name', 'joined', 'social_media_url', 'player_agent_name', 'date_of_last_contract_extension', 'on_loan_from_club_name', 'second_club_url', 'second_club_name', 'date_of_death', 'competition_id', 'competition_name', 'team_name', 'injury_reason', 'from_date', 'end_date', 'date_unix']\n",
            "Reduced Master1 shape: (3749436, 46)\n",
            "Sampled dataset shape: (50000, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = master1_small.select_dtypes(include=['object']).columns\n",
        "\n",
        "low_card = [col for col in cat_cols if master1_small[col].nunique() < 50]\n",
        "high_card = [col for col in cat_cols if col not in low_card]\n",
        "\n",
        "print(\"Low-cardinality:\", low_card)\n",
        "print(\"High-cardinality:\", high_card)\n",
        "master2 = safe_ohe(master1_small, low_card)\n",
        "print(\"After OHE:\", master2.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnVZcqw9xFSF",
        "outputId": "422a2635-6030-4130-ff77-5ba21e5ddd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Low-cardinality: ['position', 'main_position', 'foot', 'outfitter', 'contract_option', 'contract_there_expires', 'third_club_url', 'third_club_name', 'fourth_club_url', 'fourth_club_name', 'season_name_y']\n",
            "High-cardinality: ['contract_expires', 'season_name_x']\n",
            "After OHE: (50000, 161)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "for col in high_card:\n",
        "    le = LabelEncoder()\n",
        "    master2[col] = le.fit_transform(master2[col].astype(str))\n",
        "\n",
        "print(\"After Label Encoding:\", master2.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpx76gURxIl1",
        "outputId": "27aca581-acf2-427e-b12b-bab68db55088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Label Encoding: (50000, 161)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "master2_path = path + \"master_file2_preprocessed_small.csv\"\n",
        "master2.to_csv(master2_path, index=False)\n",
        "\n",
        "print(\"Master File 2 saved:\", master2_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIMrw0DtxUAV",
        "outputId": "cccdb241-18ba-4fa8-884f-d42c94409db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master File 2 saved: /content/drive/MyDrive/datasets_2/master_file2_preprocessed_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "master1 = safe_load_csv(master1_path)\n",
        "\n",
        "master1_small = master1.sample(n = 50000, random_state=42)\n",
        "\n",
        "cat_cols = master1.select_dtypes(include=['object']).columns\n",
        "low_card = [c for c in cat_cols if master1[c].nunique() < 50]\n",
        "high_card = [c for c in cat_cols if c not in low_card]\n",
        "\n",
        "print(\"Low-cardinality:\", low_card)\n",
        "print(\"High-cardinality:\", high_card)\n",
        "\n",
        "master2 = safe_ohe(master1, low_card)\n",
        "\n",
        "for col in high_card:\n",
        "    le = LabelEncoder()\n",
        "    master2[col] = le.fit_transform(master2[col].astype(str))\n",
        "\n",
        "master2_path = path + \"master_file2_preprocessed.csv\"\n",
        "master2.to_csv(master2_path, index=False)\n",
        "master2_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lbsaCrTmNuV",
        "outputId": "a3ddacd8-4e24-4157-a153-684fcb030ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: utf-8\n",
            "Low-cardinality: ['position', 'main_position', 'foot', 'outfitter', 'contract_option', 'contract_there_expires', 'third_club_url', 'third_club_name', 'fourth_club_url', 'fourth_club_name', 'season_name_y']\n",
            "High-cardinality: ['player_slug', 'player_name_x', 'player_image_url', 'name_in_home_country', 'date_of_birth', 'place_of_birth', 'country_of_birth', 'citizenship', 'current_club_name', 'joined', 'contract_expires', 'social_media_url', 'player_agent_name', 'date_of_last_contract_extension', 'on_loan_from_club_name', 'second_club_url', 'second_club_name', 'date_of_death', 'season_name_x', 'competition_id', 'competition_name', 'team_name', 'injury_reason', 'from_date', 'end_date', 'date_unix']\n"
          ]
        }
      ]
    }
  ]
}